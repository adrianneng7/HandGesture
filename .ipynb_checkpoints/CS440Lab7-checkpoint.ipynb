{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Spring 2019, Lab 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This program introduces the following concepts:\n",
    "\n",
    "*\t\ta) Reading a stream of images from a webcamera, and displaying the video (learned in lab 6)\n",
    "*\t\tb) Skin color detection (learned in lab 6)\n",
    "*\t\tc) Background differencing\n",
    "*\t\td) Visualizing motion history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from collections import deque\n",
    "bg = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skin color detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that detects whether a pixel belongs to the skin based on RGB values\n",
    "# src - the source color image\n",
    "def mySkinDetect(src):\n",
    "    # Surveys of skin color modeling and detection techniques:\n",
    "    # 1. Vezhnevets, Vladimir, Vassili Sazonov, and Alla Andreeva. \"A survey on pixel-based skin color detection techniques.\" Proc. Graphicon. Vol. 3. 2003.\n",
    "    # 2. Kakumanu, Praveen, Sokratis Makrogiannis, and Nikolaos Bourbakis. \"A survey of skin-color modeling and detection methods.\" Pattern recognition 40.3 (2007): 1106-1122.\n",
    "    dst = np.zeros((src.shape[0], src.shape[1], 1), dtype = \"uint8\")\n",
    "    for i in range(src.shape[0]):\n",
    "        for j in range(src.shape[1]):\n",
    "            #b,g,r = src[i,j]\n",
    "            b = int(src[i,j][0])\n",
    "            g = int(src[i,j][1])\n",
    "            r = int(src[i,j][2])\n",
    "            if(r>45 and g>20 and b>5 and max(r,g,b)-min(r,g,b)>15 and abs(r-g)>15 and r>g and r>b):\n",
    "                dst[i,j] = 255\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# frame-to-frame differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that does frame differencing between the current frame and the previous frame\n",
    "# prev - the previous color image\n",
    "# curr - the current color image\n",
    "# dst - the destination grayscale image where pixels are colored white if the corresponding pixel intensities in the current\n",
    "# and previous image are not the same\n",
    "def myFrameDifferencing(prev, curr):\n",
    "    # For more information on operation with arrays: \n",
    "    # http://docs.opencv.org/modules/core/doc/operations_on_arrays.html\n",
    "    \n",
    "    \n",
    "    \n",
    "    #dst = np.zeros((prev.shape[0], prev.shape[1], 1), dtype = \"uint8\")\n",
    "    dst = cv2.absdiff(prev,curr)\n",
    "    dst = cv2.cvtColor(dst, cv2.COLOR_BGR2GRAY) #convert color image to gray scale image\n",
    "    _, dst = cv2.threshold(dst, 50, 255, cv2.THRESH_BINARY) #if pixel is greater than 50, set the pixel to white (only get the second value)\n",
    "                \n",
    "            \n",
    "    return dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# motion energy templates\n",
    "* example 1: the bottom row displays a cumulative binary motion energy image sequence corresponding to the frames above\n",
    "![title](mh1.png)\n",
    "* example 2: pixel intensity is a function of the motion history at that location, where brighter values correspond to more recent motion, three actions: sit-down, arms-raise, crouch-down\n",
    "![title](mh2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that accumulates the frame differences for a certain number of pairs of frames\n",
    "# mh - vector of frame difference images\n",
    "# dst - the destination grayscale image to store the accumulation of the frame difference images\n",
    "def myMotionEnergy(mh):\n",
    "    # the window of time is 3\n",
    "    mh0 = mh[0]\n",
    "    mh1 = mh[1]\n",
    "    mh2 = mh[2]\n",
    "    dst = np.zeros((mh0.shape[0], mh0.shape[1], 1), dtype = \"uint8\")\n",
    "    for i in range(mh0.shape[0]):\n",
    "        for j in range(mh0.shape[1]):\n",
    "            if mh0[i,j] == 255 or mh1[i,j] == 255 or mh2[i,j] == 255:\n",
    "                dst[i,j] = 255\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgbg = cv2.bgsegm.createBackgroundSubtractorMOG(0,5,0.5,0)\n",
    "#history=0,nmixtures = 5, backgroundRatio = 0.7, dnoiseSigma = 0\n",
    "def backgroundDifferencing(src):\n",
    "    \n",
    "    #while(1):\n",
    "        #ret, frame = src.read()\n",
    "    fgmask = fgbg.apply(src)\n",
    "        #cv.imshow('frame',fgmask)\n",
    "        #k = cv.waitKey(30) & 0xff\n",
    "        #if k == 27:\n",
    "         #   break\n",
    "    return fgmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backgroundDifferencing2(src, weight):\n",
    "    global bg\n",
    "    if bg is None:\n",
    "        bg = src.copy().astype(\"float\") \n",
    "    cv2.accumulateWeighted(src,bg,weight)\n",
    "    return bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def templateMatching2(curr):\n",
    "    img2 = curr.copy()\n",
    "    template = cv2.imread('template2.png',0)\n",
    "    dst = np.zeros((curr.shape[0], curr.shape[1], 1), dtype = \"uint8\")\n",
    "\n",
    "    templateresized = cv2.resize(template, (60, 60)) \n",
    "    \n",
    "    w, h = templateresized.shape[::-1]\n",
    "    img = img2.copy()\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    #print(img)\n",
    "    res = cv2.matchTemplate(img,templateresized,cv2.TM_CCOEFF)\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "    top_left = max_loc\n",
    "    bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "    dst = cv2.rectangle(img,top_left, bottom_right, 255, 2)\n",
    "    return dst \n",
    "    plt.subplot(121),plt.imshow(res,cmap = 'gray')\n",
    "    plt.title('Matching Result'), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(122),plt.imshow(img,cmap = 'gray')\n",
    "    plt.title('Detected Point'), plt.xticks([]), plt.yticks([])\n",
    "    plt.suptitle('cv2.TM_CCOEFF')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def templateMatching3(curr):    \n",
    "    \n",
    "    img2 = myMotionEnergy(curr)\n",
    "    #img2.astype(np.float32)\n",
    "    #print(img2)\n",
    "    template = cv2.imread('template.png',0)\n",
    "    #print(template)\n",
    "    #template.astype(np.float32)\n",
    "    #dst = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\n",
    "    ret,dst = cv2.threshold(template,127,255,cv2.THRESH_BINARY)\n",
    "    #print(dst)\n",
    "    \n",
    "    templateresized = cv2.resize(dst, (60, 60)) \n",
    "    # All the 6 methods for comparison in a list\n",
    "    methods = ['cv2.TM_CCOEFF', 'cv2.TM_CCOEFF_NORMED', 'cv2.TM_CCORR',\n",
    "                'cv2.TM_CCORR_NORMED', 'cv2.TM_SQDIFF', 'cv2.TM_SQDIFF_NORMED']\n",
    "\n",
    "    for meth in methods:\n",
    "        img = img2.copy()\n",
    "        #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = img.astype(np.float32)\n",
    "        templateresied = templateresized.astype(np.float32)\n",
    "        \n",
    "        method = eval(meth)\n",
    "        w, h = templateresized.shape[::-1]\n",
    "        # Apply template Matching\n",
    "        res = cv2.matchTemplate(img,templateresized,method)\n",
    "        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "\n",
    "        # If the method is TM_SQDIFF or TM_SQDIFF_NORMED, take minimum\n",
    "        if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n",
    "            top_left = min_loc\n",
    "        else:\n",
    "            top_left = max_loc\n",
    "        bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "\n",
    "        return cv2.rectangle(img,top_left, bottom_right, 255, 2)\n",
    "        \n",
    "        \n",
    "\n",
    "        plt.subplot(121),plt.imshow(res,cmap = 'gray')\n",
    "        plt.title('Matching Result'), plt.xticks([]), plt.yticks([])\n",
    "        plt.subplot(122),plt.imshow(img,cmap = 'gray')\n",
    "        plt.title('Detected Point'), plt.xticks([]), plt.yticks([])\n",
    "        plt.suptitle(meth)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manualtemplatematch(curr):\n",
    "    img = myMotionEnergy(curr)\n",
    "    ret,dst = cv2.threshold(template,127,255,cv2.THRESH_BINARY)\n",
    "    templateresized = cv2.resize(dst, (curr.shape[0], curr.shape[1]))\n",
    "    n = 0\n",
    "    for i in range(curr.shape[0]):\n",
    "        for j in range(curr.shape[1]):\n",
    "            if img[i,j] == templateresized[i,j]:\n",
    "                n +=1\n",
    "    if n/22500 >=.7:\n",
    "        return True\n",
    "    return False\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(src, threshold = 50):\n",
    "    global bg \n",
    "    diff = cv2.absdiff(bg.astype(\"uint8\"),src)\n",
    "    thresholded = cv2.threshold(diff,threshold,255,cv2.THRESH_BINARY)[1]\n",
    "    (_,cnts, _) = cv2.findContours(mySkinDetect(src), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(cnts)==0:\n",
    "        return\n",
    "    else:\n",
    "        segmented = max(cnts, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(segmented)\n",
    "        cv2.rectangle(src, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        return (cv2.drawContours(src, segmented, -1, (255, 255, 0), 1))\n",
    "        #return (thresholded, segmented)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundingBox(curr):\n",
    "    \n",
    "    # find contours and get the external one\n",
    "    #image, contours, hier = cv2.findContours(mySkinDetect(curr), cv2.RETR_TREE,\n",
    "     #               cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    (_,cnts, _) = cv2.findContours(mySkinDetect(curr), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(cnts)==0:\n",
    "        return\n",
    "    else:\n",
    "        segmented = max(cnts, key=cv2.contourArea)\n",
    "        return segmented\n",
    "        #x, y, w, h = cv2.boundingRect(segmented)\n",
    "        #cv2.rectangle(curr, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        #(cv2.drawContours(curr, segmented, -1, (255, 255, 0), 1))\n",
    "    #return segmented\n",
    "\n",
    "    # with each contour, draw boundingRect in green\n",
    "    # a minAreaRect in red and\n",
    "    # a minEnclosingCircle in blue\n",
    "    #for c in contours:\n",
    "        # get the bounding rect\n",
    "        #x, y, w, h = cv2.boundingRect(c)\n",
    "        # draw a green rectangle to visualize the bounding rect\n",
    "        #cv2.rectangle(curr, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        # get the min area rect\n",
    "        #rect = cv2.minAreaRect(c)\n",
    "     \n",
    "        #box = cv2.boxPoints(rect)\n",
    "        # convert all coordinates floating point values to int\n",
    "     \n",
    "        #box = np.int0(box)\n",
    "       \n",
    "\n",
    "    #return (cv2.drawContours(curr, contours, -1, (255, 255, 0), 1))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(curr, segmented):\n",
    "    #fnd the convex hull of the segmented hand region\n",
    "    chull = cv2.convexHull(segmented)\n",
    "    thresholded = mySkinDetect(curr)\n",
    "    # find the most extreme points in the convex hull\n",
    "    extreme_top    = tuple(chull[chull[:, :, 1].argmin()][0])\n",
    "    extreme_bottom = tuple(chull[chull[:, :, 1].argmax()][0])\n",
    "    extreme_left   = tuple(chull[chull[:, :, 0].argmin()][0])\n",
    "    extreme_right  = tuple(chull[chull[:, :, 0].argmax()][0])\n",
    "    \n",
    "\n",
    "    # find the center of the palm\n",
    "    cX = (extreme_left[0] + extreme_right[0]) / 2\n",
    "    cY = (extreme_top[1] + extreme_bottom[1]) / 2\n",
    "    dist1 = ((cX-extreme_left[0])**2 + (cY-extreme_left[1])**2)**0.5\n",
    "    dist2 = ((cX-extreme_right[0])**2 + (cY-extreme_right[1])**2)**0.5\n",
    "    dist3 = ((cX-extreme_top[0])**2 + (cY-extreme_top[1])**2)**0.5\n",
    "    dist4 = ((cX-extreme_bottom[0])**2 + (cY-extreme_bottom[1])**2)**0.5\n",
    "    # find the maximum euclidean distance between the center of the palm\n",
    "    # and the most extreme points of the convex hull\n",
    "    x = np.array([(cX,cY)])\n",
    "    y = np.array([extreme_left, extreme_right, extreme_top, extreme_bottom])\n",
    "    \n",
    "    distance = np.array ([[dist1], [dist2], [dist3], [dist4]])[0]\n",
    "    maximum_distance = distance[distance.argmax()]\n",
    "    \n",
    "    # calculate the radius of the circle with 80% of the max euclidean distance obtained\n",
    "    radius = int(0.8 * maximum_distance)\n",
    "    \n",
    "    # find the circumference of the circle\n",
    "    circumference = (2 * np.pi * radius)\n",
    "\n",
    "    # take out the circular region of interest which has \n",
    "# the palm and the fingers\n",
    "    circular_roi = np.zeros(thresholded.shape[:2], dtype=\"uint8\")\n",
    "    # draw the circular ROI\n",
    "    cv2.circle(circular_roi, (int(cX), int(cY)), int(radius), 255, 1)\n",
    "    \n",
    "\n",
    "\t# take bit-wise AND between thresholded hand using the circular ROI as the mask\n",
    "\t# which gives the cuts obtained using mask on the thresholded hand image\n",
    "    circular_roi = cv2.bitwise_and(thresholded, thresholded, mask=circular_roi)\n",
    "\n",
    "\t# compute the contours in the circular ROI\n",
    "    (_, cnts, _) = cv2.findContours(circular_roi.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "\t# initalize the finger count\n",
    "    count = 0\n",
    "\n",
    "\t# loop through the contours found\n",
    "    for c in cnts:\n",
    "\t\t# compute the bounding box of the contour\n",
    "        (x, y, w, h) = cv2.boundingRect(c)\n",
    "\n",
    "\t\t# increment the count of fingers only if -\n",
    "\t\t# 1. The contour region is not the wrist (bottom area)\n",
    "\t\t# 2. The number of points along the contour does not exceed\n",
    "\t\t#     25% of the circumference of the circular ROI\n",
    "        if ((cY + (cY * 0.25)) > (y + h)) and ((circumference * 0.25) > c.shape[0]):\n",
    "            count += 1\n",
    "    if (manualtemplatematch(curr)) == True:\n",
    "        return (\"Waving\")\n",
    "    else:\n",
    "        if count == 2: \n",
    "            return (\"Scissor\")\n",
    "        if count >=5 :\n",
    "            return (\"Paper\")\n",
    "        if count == 0:\n",
    "            return (\"Rock\")\n",
    "    return (\"No Gesture Detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motion(mh,segmented):\n",
    "    if count(mh[0],segmented) == \"Paper\":\n",
    "        if count (mh[1],segmented) == \"Paper\":\n",
    "            if count (mh[2],segmented) == \"Paper\":\n",
    "                return (\"Waving\")\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # a) Reading a stream of images from a webcamera, and displaying the video\n",
    "    # open the video camera no. 0/modules/highgui/doc/reading_and_writing_images_and_video.html\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    #if not successful, exit program\n",
    "    if not cap.isOpened():\n",
    "        print(\"Cannot open the video cam\")\n",
    "        return -1\n",
    "\n",
    "    # read a new frame from video\n",
    "    success, prev_frame = cap.read()\n",
    "    \n",
    "    #if not successful, exit program\n",
    "    if not success:\n",
    "        print(\"Cannot read a frame from video stream\")\n",
    "        return -1\n",
    "    cv2.namedWindow(\"frame\", cv2.WINDOW_AUTOSIZE)\n",
    "    \n",
    "    prev_frame = cv2.resize(prev_frame,(150,150))\n",
    "    fMH1 = np.zeros((prev_frame.shape[0], prev_frame.shape[1], 1), dtype = \"uint8\")\n",
    "    fMH2 = fMH1.copy()\n",
    "    fMH3 = fMH1.copy()\n",
    "    myMotionHistory = deque([fMH1, fMH2, fMH3])\n",
    "\n",
    "    \n",
    "    while(True):\n",
    "        #read a new frame from video\n",
    "        success, curr_frame = cap.read()\n",
    "        curr_frame = cv2.resize(curr_frame,(150,150))\n",
    "        \n",
    "        if not success:\n",
    "            print(\"Cannot read a frame from video stream\")\n",
    "            break\n",
    "        curr_frame = cv2.flip(curr_frame,1)\n",
    "        \n",
    "        #if num_frames<30:\n",
    "         #   backgroundDifferencing2(gray, weight)\n",
    "        #if num_frames == 30:\n",
    "         #   print(\"you can move\")\n",
    "        #else:\n",
    "            #hand = segment(gray)\n",
    "            #if hand is not None:\n",
    "                #(thresholded, segmented) = hand\n",
    "                #cv2.drawContours(clone, [segmented + (right,top)], -1,(0,0,255))\n",
    "               # cv2.imshow(\"Thresholded\", thresholded)\n",
    "        #cv2.imshow(\"background\", bg)\n",
    "        #cv2.rectangle(clone, (left, top), (right, bottom), (0,255,0),2 )\n",
    "        #num_frames += 1\n",
    "        #cv2.imshow(\"hand\" , clone)\n",
    "\n",
    "        cv2.imshow('frame',curr_frame)\n",
    "\n",
    "        # b) Skin color detection\n",
    "        #mySkin = mySkinDetect(curr_frame)\n",
    "        #cv2.imshow('mySkinDetect',mySkin)\n",
    "\n",
    "        #Background differencing\n",
    "        #bd = backgroundDifferencing(curr_frame)\n",
    "        #cv2.imshow('myBackgroundDifferencing', bd)\n",
    "        \n",
    "        # c) Frame by frame differencing\n",
    "        frameDest = myFrameDifferencing(prev_frame, curr_frame)\n",
    "        cv2.imshow('myFrameDifferencing',frameDest)\n",
    "        \n",
    "        \n",
    "         #template matching\n",
    "        #tm = templateMatching2(curr_frame)\n",
    "        #cv2.imshow('templateMatching', tm)\n",
    "        \n",
    "        #bounding box\n",
    "        segmented = boundingBox(curr_frame)\n",
    "        x, y, w, h = cv2.boundingRect(segmented)\n",
    "        bound = cv2.rectangle(curr_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        bound =(cv2.drawContours(curr_frame, segmented, -1, (255, 255, 0), 1))\n",
    "        \n",
    "        \n",
    "        fingers = count(curr_frame, segmented)\n",
    "     \n",
    "        bound = cv2.putText(curr_frame, str(fingers), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, .5, (0,0,255), 2)\n",
    "        cv2.imshow('myBoundingBox', bound)\n",
    "\n",
    "        # d) Visualizing motion history\n",
    "        myMotionHistory.popleft()\n",
    "        myMotionHistory.append(frameDest)\n",
    "        myMH = myMotionEnergy(myMotionHistory)\n",
    "        cv2.imshow('myMotionHistory',myMH)\n",
    "\n",
    "        prev_frame = curr_frame\n",
    "        \n",
    "        #motion\n",
    "        \n",
    "        \n",
    "        \n",
    "        # wait for 'q' key press. If 'q' key is pressed, break loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot read a frame from video stream\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
